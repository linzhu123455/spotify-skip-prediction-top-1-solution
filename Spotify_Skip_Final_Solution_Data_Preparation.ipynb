{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation of the song metadata file, the track_id is encoded as numerical values to save space.\n",
    "\n",
    "import numpy as np\n",
    "#import pylab as Plot\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "#data_path = 'D:/skip spotify code/track_features/'\n",
    "#data_path = '/home/sc4/skip/track_features/'\n",
    "data_path = 'Skip_Data/'\n",
    "\n",
    "song_fea_0 = pd.read_csv(data_path+'tf_000000000000.csv')\n",
    "song_fea_1 = pd.read_csv(data_path+'tf_000000000001.csv')\n",
    "\n",
    "song_fea_0 = pd.concat((song_fea_0,song_fea_1))\n",
    "\n",
    "le = LabelEncoder()\n",
    "#\n",
    "song_fea_0['track_id'] = le.fit_transform(song_fea_0['track_id'])\n",
    "\n",
    "joblib.dump(le, 'le_track_id.pkl')\n",
    "\n",
    "song_fea_0['mode'] = le.fit_transform(song_fea_0['mode'])\n",
    "\n",
    "song_fea_0.to_parquet('spotify_song_fea.parquet')\n",
    "\n",
    "song_fea_1 = []\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of the training session csv files to parquet files for smaller size and fast loading, the track_ids and session ids are encoded as numerical values to save space. The generated files are saved in the 'Skip_Data/' folder.\n",
    "# If done correcly, the name of the generated parquet files should be something like \"log_3_20180918_000000000000.csv.parquet\"\n",
    "\n",
    "import numpy as np\n",
    "import pylab as Plot\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "le_track_id = joblib.load('le_track_id.pkl')\n",
    "\n",
    "data_path = 'Skip_Data/'\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "dirs = os.listdir( 'train_fold/' )\n",
    "count = 0\n",
    "for json_file in dirs:    \n",
    "    print(0, count/len(dirs))\n",
    "    \n",
    "    train_data = pd.read_csv('train_fold/'+json_file)\n",
    "    \n",
    "    le_session = LabelEncoder()\n",
    "\n",
    "    train_data['session_id'] = le_session.fit_transform(train_data['session_id'])\n",
    "\n",
    "    train_data['track_id_clean'] = le_track_id.transform(train_data['track_id_clean'])\n",
    "\n",
    "    train_data.to_parquet(data_path+json_file+'.parquet')\n",
    "     \n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of the training session csv files to parquet files for smaller size and fast loading, the track_ids and session ids are encoded as numerical values to save space. The generated files are saved in the 'Skip_Data/' folder.\n",
    "# There are two kinds of test files, namely files with names that start with \"log_input\" or \"log_prehistory\". If this part of code is executed correctly, the names of the generated parquet files should be correspondingly be something like \"pred_20180718.parquet\" and \"prehist_20180810.parquet\" for \n",
    "\n",
    "\n",
    "import hickle as hkl\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "le_track_id = joblib.load('le_track_id.pkl')\n",
    "\n",
    "test_files = glob.glob('test_fold/log_input_*_000000000000.csv')\n",
    "\n",
    "for i in tqdm(range(len(test_files))):\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    file = pd.read_csv(test_files[i])\n",
    "    \n",
    "    prefix = 'test_fold/log_prehistory'\n",
    "    \n",
    "    pre_file = pd.read_csv(prefix + test_files[i][33:])\n",
    "    \n",
    "    file['session_id'] = le.fit_transform(file['session_id'])\n",
    "    \n",
    "    pre_file['session_id'] = le.transform(pre_file['session_id'])\n",
    "    \n",
    "    pre_file['track_id_clean'] = le_track_id.transform(pre_file['track_id_clean'])\n",
    "    file['track_id_clean'] = le_track_id.transform(file['track_id_clean'])\n",
    "    \n",
    "    file.to_parquet('test_pred/'+ 'pred_'+test_files[i][34:42]+'.parquet')\n",
    "    \n",
    "    pre_file.to_parquet('test_pred/'+ 'prehist_'+test_files[i][34:42]+'.parquet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation of the file for Glove training.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pylab as Plot\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import glob\n",
    "\n",
    "data_path = 'Skip_Data/'\n",
    "\n",
    "train_files = glob.glob(data_path + '*.csv.parquet')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#data_path = 'D:/skip spotify code/track_features/'\n",
    "#data_path = '/home/sc4/skip/track_features/'\n",
    "\n",
    "le_track_id = joblib.load('le_track_id.pkl')\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "#dirs = dirs[0:200]\n",
    "\n",
    "all_data = []\n",
    "\n",
    "count = 0\n",
    "for file in train_files:    \n",
    "    print(0, count/len(train_files))\n",
    "    \n",
    "    train_data = pd.read_parquet(file)\n",
    "    \n",
    "    cols = ['session_id', 'session_position', 'session_length', 'track_id_clean']\n",
    "    \n",
    "    train_data = train_data[cols]\n",
    "    \n",
    "    train_data['track_id_clean'] = train_data['track_id_clean'] + 1\n",
    "    \n",
    "    raw_data = np.array(train_data.values)*1\n",
    "\n",
    "    raw_data = raw_data.astype(np.int)    \n",
    "    \n",
    "    n_session = np.max(train_data['session_id'])+1\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    data = np.zeros((n_session*30))\n",
    "    \n",
    "    data[raw_data[:,0].astype(np.int)*30+raw_data[:,1].astype(np.int)-1] = raw_data[:,3]\n",
    "\n",
    "    if count == 0:\n",
    "        all_data = data\n",
    "    else:\n",
    "        all_data = np.concatenate((all_data,data))\n",
    "     \n",
    "    count = count + 1\n",
    "\n",
    "np.savetxt('glove_data.txt',all_data,newline=' ', delimiter = ' ', fmt='%i') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After learning the Glove embedding, let the embedding txt file be named as 'vectors_150.txt', this part of code convert the txt file to numpy format.\n",
    "\n",
    "import numpy as np\n",
    "import pylab as Plot\n",
    "import pandas as pd\n",
    "import gc\n",
    "import hickle as hkl\n",
    "#from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_coefs(word,*arr): return (word), np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open('vectors_150.txt'))\n",
    "\n",
    "song_embedding_matrix = np.zeros((3706389,150))\n",
    "\n",
    "keys = embeddings_index.keys()\n",
    "\n",
    "for i in range(0,song_embedding_matrix.shape[0]):\n",
    "        tmp = embeddings_index.get(str(i))\n",
    "        if tmp is not None:\n",
    "            song_embedding_matrix[i,:] = tmp\n",
    "            \n",
    "hkl.dump(song_embedding_matrix, 'song_embedding_matrix_150.hkl', mode='w', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
